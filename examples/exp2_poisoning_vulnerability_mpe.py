import os
import sys
import argparse
import pickle
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from copy import deepcopy
from sklearn.metrics import accuracy_score

sys.path.append("..")
from lib.datasets import get_dataset
from lib.models import get_model
from lib.utils import get_quick_loader, predict_train2, predict_test, train_network
from lib.variances import get_pred_vars_laplace

from torch.nn import functional as F
from torch.nn import CrossEntropyLoss

def build_model(model_name, nc, ds_train, device, seed, verbose=True):
    data_arr = getattr(ds_train, 'data', None)
    if data_arr is None:
        raise RuntimeError("Dataset has no attribute .data to infer input shape")
    arr = np.array(data_arr)
    # arr may be (N,H,W,C) or (N,C,H,W)
    if arr.ndim == 4 and arr.shape[-1] in (1,3):
        H, W, C = arr.shape[1], arr.shape[2], arr.shape[3]
    elif arr.ndim == 4 and arr.shape[1] in (1,3):
        C, H, W = arr.shape[1], arr.shape[2], arr.shape[3]
    else:
        x0, _ = ds_train[0]
        if isinstance(x0, torch.Tensor):
            C, H, W = x0.shape
        else:
            raise RuntimeError("Cannot infer input shape from dataset")
    input_size_try = C * H * W

    model = None
    err_msgs = []
    try:
        model = get_model(model_name, nc, input_size_try, device, seed)
        if verbose: print(f"Built model with signature get_model(name, nc, input_size, device, seed)")
    except Exception as e1:
        err_msgs.append(str(e1))
    if model is None:
        try:
            model = get_model(model_name, nc, input_shape=(C,H,W), device=device, seed=seed)
            if verbose: print(f"Built model with signature get_model(name, nc, input_shape=..., device=..., seed=...)")
        except Exception as e2:
            err_msgs.append(str(e2))
    if model is None:
        try:
            model = get_model(model_name, nc)
            if verbose: print("Built model with signature get_model(name, nc)")
        except Exception as e3:
            err_msgs.append(str(e3))
    if model is None:
        try:
            model = get_model(model_name, nc, device=device)
            if verbose: print("Built model with signature get_model(name, nc, device=device)")
        except Exception as e4:
            err_msgs.append(str(e4))

    if model is None:
        raise RuntimeError("Failed to build model. Tried signatures; errors:\n" + "\n".join(err_msgs))

    # ensure model on device
    model = model.to(device)
    # seed model weights if possible (some get_model already sets seed)
    try:
        torch.manual_seed(seed)
        if device.startswith('cuda'):
            torch.cuda.manual_seed_all(seed)
    except Exception:
        pass

    return model

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument('--name_exp', default='exp2_cifar10_poisoning_mpe', type=str)
    p.add_argument('--dataset', default='CIFAR10', choices=['CIFAR10'])
    p.add_argument('--model', default='cnn_deepobs')
    p.add_argument('--bs', default=256, type=int)
    p.add_argument('--bs_jacs', default=20, type=int)
    p.add_argument('--epochs', default=250, type=int)
    p.add_argument('--lr', default=0.01, type=float)
    p.add_argument('--lrmin', default=1e-4, type=float)
    p.add_argument('--delta', default=250, type=float)
    p.add_argument('--k_fracs', nargs='+', type=float, default=[0.01, 0.03, 0.05, 0.07, 0.09])
    # p.add_argument('--eps_fgsm', default=4.0/255.0, type=float)
    p.add_argument('--n_random_repeats', default=5, type=int)
    p.add_argument('--seed', default=1, type=int)
    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()
    print(args)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print('device', device)

    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    ds_train, ds_test, transform_train = get_dataset(args.dataset, return_transform=True)
    n_train = len(ds_train)
    nc = int(max(ds_train.targets)) + 1

    net = build_model(args.model, nc, ds_train, device, args.seed)
    trainloader = get_quick_loader(DataLoader(ds_train, batch_size=args.bs, shuffle=True))
    net, losses = train_network(net, trainloader, args.lr, args.lrmin, args.epochs, n_train, args.delta)
    baseline_test_acc, _ = predict_test(net, DataLoader(ds_test, batch_size=args.bs, shuffle=False), nc,
                                       torch.asarray(ds_test.targets), device)
    print(f"Baseline test acc: {100*baseline_test_acc:.2f}%")

    # compute sensitivities on train set (same pipeline as in validation script)
    trainloader_eval = DataLoader(ds_train, batch_size=args.bs, shuffle=False)
    trainloader_vars = DataLoader(ds_train, batch_size=args.bs_jacs, shuffle=False)
    residuals, probs, lambdas, train_acc, train_nll = predict_train2(net, trainloader_eval, nc,
                                                                     torch.asarray(ds_train.targets), device)
    vars_list = get_pred_vars_laplace(net, trainloader_vars, args.delta, nc, version='kfac', device=device)
    sensitivities = np.asarray(residuals) * np.asarray(lambdas) * np.asarray(vars_list)
    sensitivities = np.sum(np.abs(sensitivities), axis=-1)
    ranking = np.argsort(-sensitivities)  # descending

    # store results
    results = {'baseline_test_acc': float(baseline_test_acc), 'k_runs': {}}
    os.makedirs('pickles', exist_ok=True)

    def retrain_and_eval(ds_mod, seed_local):
        torch.manual_seed(seed_local)
        np.random.seed(seed_local)
        trainloader_mod = get_quick_loader(DataLoader(ds_mod, batch_size=args.bs, shuffle=True))
        net_mod = build_model(args.model, nc, ds_train, device, seed_local)
        net_mod, _ = train_network(net_mod, trainloader_mod, args.lr, args.lrmin, args.epochs, n_train, args.delta)
        test_acc, _ = predict_test(net_mod, DataLoader(ds_test, batch_size=args.bs, shuffle=False), nc,
                                  torch.asarray(ds_test.targets), device)
        return float(test_acc)

    for k_frac in args.k_fracs:
        k = int(round(n_train * float(k_frac)))
        print(f"\n=== k_frac={k_frac} => k={k} ===")
        results['k_runs'][k_frac] = {}

        # A: flip labels top-k
        idx_top_k = ranking[:k]
        ds_flip_top = deepcopy(ds_train)
        targets_topflip = np.array(ds_flip_top.targets).copy()
        rng = np.random.default_rng(args.seed + 123)
        for idx in idx_top_k:
            orig = int(targets_topflip[idx])
            choices = list(range(nc)); choices.remove(orig)
            targets_topflip[idx] = int(rng.choice(choices))
        ds_flip_top.targets = torch.asarray(targets_topflip)
        acc_flip_top = retrain_and_eval(ds_flip_top, args.seed + 1)
        drop_flip_top = float(baseline_test_acc) - acc_flip_top
        print(f"Top-k label flip: test_acc={100*acc_flip_top:.2f}%, drop={100*drop_flip_top:.2f}%")
        results['k_runs'][k_frac]['top_flip'] = {'test_acc': acc_flip_top, 'drop': drop_flip_top, 'k': k}

        # B: flip labels random-k (multiple repeats average)
        accs_random = []
        for r in range(args.n_random_repeats):
            rng = np.random.default_rng(args.seed + 100 + r)
            idx_rand = rng.choice(n_train, size=k, replace=False)
            ds_rand = deepcopy(ds_train)
            targets_rand = np.array(ds_rand.targets).copy()
            rng2 = np.random.default_rng(args.seed + 200 + r)
            for idx in idx_rand:
                orig = int(targets_rand[idx])
                choices = list(range(nc)); choices.remove(orig)
                targets_rand[idx] = int(rng2.choice(choices))
            ds_rand.targets = torch.asarray(targets_rand)
            acc_r = retrain_and_eval(ds_rand, args.seed + 10 + r)
            accs_random.append(acc_r)
            print(f" Random flip repeat {r}: test_acc={100*acc_r:.2f}%")
        acc_random_mean = float(np.mean(accs_random))
        drop_random = float(baseline_test_acc) - acc_random_mean
        results['k_runs'][k_frac]['random_flip'] = {'test_accs': accs_random, 'test_acc_mean': acc_random_mean, 'drop': drop_random, 'k': k}
        print(f" Random flips mean: test_acc={100*acc_random_mean:.2f}%, drop={100*drop_random:.2f}%")

    # save
    with open(f'pickles/{args.name_exp}_results.pkl', 'wb') as f:
        pickle.dump(results, f)

    print("Done. Results saved to pickles/")
